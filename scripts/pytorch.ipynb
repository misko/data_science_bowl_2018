{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import math\n",
    "from functools import reduce\n",
    "\n",
    "import cv2\n",
    "from glob import glob\n",
    "\n",
    "import skimage\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "%matplotlib inline\n",
    "from skimage.color import rgb2grey, rgb2hsv, hsv2rgb, grey2rgb, rgba2rgb\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import math\n",
    "from functools import reduce\n",
    "\n",
    "import cv2\n",
    "from glob import glob\n",
    "\n",
    "import skimage\n",
    "from skimage.io import imread\n",
    "from skimage import img_as_float, img_as_ubyte\n",
    "from skimage.morphology import reconstruction\n",
    "from skimage.util import invert\n",
    "from scipy import ndimage as ndi\n",
    "from skimage.morphology import watershed\n",
    "from skimage.feature import peak_local_max\n",
    "from skimage.color import label2rgb\n",
    "from skimage.filters import gaussian\n",
    "\n",
    "from skimage.morphology import erosion, dilation, binary_dilation, opening, closing, white_tophat\n",
    "from skimage.morphology import disk\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.morphology import label\n",
    "\n",
    "from utils import show_img, plot_img_and_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform = reload(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataset.py\n"
     ]
    }
   ],
   "source": [
    "#%%writefile dataset.py\n",
    "#%pycat dataset.py\n",
    "\n",
    "from __future__ import division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import math\n",
    "from functools import reduce\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import skimage\n",
    "from skimage.io import imread\n",
    "from skimage import img_as_float, img_as_ubyte\n",
    "\n",
    "import sklearn\n",
    "\n",
    "\n",
    "class NucleusDataset(Dataset):\n",
    "    \"\"\"Nucleus dataset.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def read_and_stack(in_img_list):\n",
    "        # return np.sum(np.stack([i*(imread(c_img)>0) for i, c_img in\n",
    "        # enumerate(in_img_list, 1)], 0), 0)\n",
    "        r = (reduce(\n",
    "                np.bitwise_or, [\n",
    "                    np.asarray(Image.open(c_img)) for c_img in iter(in_img_list)])).astype(np.uint8)\n",
    "        #r = r / r.max()\n",
    "        return r\n",
    "\n",
    "    @staticmethod\n",
    "    def read_image(in_img_list):\n",
    "        #img = img_as_float(rgb2hsv(rgba2rgb(io.imread(in_img_list[0]))))\n",
    "        img = Image.open(in_img_list[0])\n",
    "        return np.array(img.convert('RGB')), img.size\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            root_dir=None,\n",
    "            stage_name='stage1',\n",
    "            group_name='train',\n",
    "            transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        if root_dir is None:\n",
    "            return\n",
    "\n",
    "        all_images = glob(\n",
    "            os.path.join(\n",
    "                root_dir,\n",
    "                stage_name +\n",
    "                '_*',\n",
    "                '*',\n",
    "                '*',\n",
    "                '*'))\n",
    "        \n",
    "        img_df = pd.DataFrame({'path': all_images})\n",
    "\n",
    "        def img_id(in_path): return in_path.split('/')[-3]\n",
    "\n",
    "        def img_type(in_path): return in_path.split('/')[-2]\n",
    "\n",
    "        def img_group(in_path): return in_path.split('/')[-4].split('_')[1]\n",
    "\n",
    "        def img_stage(in_path): return in_path.split('/')[-4].split('_')[0]\n",
    "        img_df['id'] = img_df['path'].map(img_id)\n",
    "        img_df['type'] = img_df['path'].map(img_type)\n",
    "        img_df['group'] = img_df['path'].map(img_group)\n",
    "        img_df['stage'] = img_df['path'].map(img_stage)\n",
    "        self.img_df = img_df\n",
    "\n",
    "        data_df = img_df.query('group==\"%s\"' % group_name)\n",
    "        data_rows = []\n",
    "        group_cols = ['stage', 'id']\n",
    "        for n_group, n_rows in data_df.groupby(group_cols):\n",
    "            c_row = {\n",
    "                col_name: col_value for col_name,\n",
    "                col_value in zip(\n",
    "                    group_cols,\n",
    "                    n_group)}\n",
    "            c_row['masks'] = n_rows.query('type == \"masks\"')[\n",
    "                'path'].values.tolist()\n",
    "            c_row['images'] = n_rows.query('type == \"images\"')[\n",
    "                'path'].values.tolist()\n",
    "            data_rows += [c_row]\n",
    "\n",
    "        data_df = pd.DataFrame(data_rows)\n",
    "\n",
    "        ret = data_df['images'].map(self.read_image)\n",
    "        #(data_df['images'], data_df['format'], data_df['mode'], data_df['size']) = ([x[i] for x in ret] for i in range(4))\n",
    "        (data_df['images'], data_df['size']) = ([x[i] for x in ret] for i in range(2))\n",
    "    \n",
    "        data_df['masks'] = data_df['masks'].map(\n",
    "            self.read_and_stack).map(\n",
    "            lambda x: x.astype(int))\n",
    "\n",
    "        # print data_df.describe()\n",
    "        self.data_df = data_df\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        sample = self.data_df[\"images\"].iloc[idx]\n",
    "        masks =  self.data_df[\"masks\"].iloc[idx]\n",
    "        # insert one dummy channel, to be consistent with output\n",
    "        masks = np.expand_dims(masks, 2)\n",
    "        if self.transform:\n",
    "            sample, masks = self.transform(sample, masks)\n",
    "\n",
    "        return sample, masks\n",
    "\n",
    "# https://discuss.pytorch.org/t/feedback-on-pytorch-for-kaggle-competitions/2252/4\n",
    "\n",
    "    def train_test_split(self, **options):\n",
    "        \"\"\" Return a list of splitted indices from a DataSet.\n",
    "        Indices can be used with DataLoader to build a train and validation set.\n",
    "\n",
    "        Arguments:\n",
    "            A Dataset\n",
    "            A test_size, as a float between 0 and 1 (percentage split) or as an int (fixed number split)\n",
    "            Shuffling True or False\n",
    "            Random seed\n",
    "        \"\"\"\n",
    "    \n",
    "        df_train, df_test = sklearn.model_selection.train_test_split(self.data_df, **options)\n",
    "        dset_train = NucleusDataset(transform=self.transform)\n",
    "        dset_train.data_df = df_train\n",
    "    \n",
    "        dset_test = NucleusDataset(transform=self.transform)\n",
    "        dset_test.data_df = df_test\n",
    "    \n",
    "        return dset_train, dset_test\n",
    "    \n",
    "    #length = len(dataset)\n",
    "    #indices = list(range(1, length))\n",
    "\n",
    "    #if shuffle:\n",
    "    #    random.seed(random_seed)\n",
    "    #    random.shuffle(indices)\n",
    "\n",
    "    #if isinstance(test_size, float):\n",
    "    #    split = int(math.floor(test_size * length + 0.5))\n",
    "    #elif isinstance(test_size, int):\n",
    "    #    split = test_size\n",
    "    #else:\n",
    "    #    raise ValueError('%s should be an int or a float' % str)\n",
    "    #return indices[split:], indices[:split]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transform\n",
    "from transform import random_rotate90_transform2\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "\n",
    "#Note: for image and mask, there is no compatible solution that can use transforms.Compse(), see https://github.com/pytorch/vision/issues/9\n",
    "#transformations = transforms.Compose([random_rotate90_transform2(),transforms.ToTensor(),])\n",
    "\n",
    "def train_transform(img, mask):\n",
    "    #print img.shape\n",
    "    #print mask.shape\n",
    "    #print type(img)\n",
    "    img, mask = random_rotate90_transform2(img, mask, 0.5)\n",
    "    #print 'after'\n",
    "    #print img.shape\n",
    "    #print mask.shape\n",
    "    #print type(mask)\n",
    "    # HACK\n",
    "    if len(mask.shape) == 2:\n",
    "      mask = np.expand_dims(mask, 2)\n",
    "    #img = torch.from_numpy(np.transpose(img,(2,0,1))).float() \n",
    "    #mask = torch.from_numpy(np.transpose(mask,(2,0,1))).float()\n",
    "    img = TT.ToTensor()(img)\n",
    "    mask = TT.ToTensor()(mask)\n",
    "    return img, mask\n",
    "\n",
    "dsb_data_dir = os.path.join('..', '..', 'input')\n",
    "stage_name = 'stage1'\n",
    "#dset = NucleusDataset(dsb_data_dir, stage_name,transform=train_transform)\n",
    "\n",
    "#dset_save = dset\n",
    "#train_idx, valid_idx = train_valid_split(dset,test_size=0.05,random_seed=1,shuffle=True)\n",
    "#train_sampler = SubsetRandomSampler(train_idx)\n",
    "#valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "#train_loader = DataLoader(dset,batch_size=1,sampler=train_sampler,num_workers=4)\n",
    "#valid_loader = DataLoader(dset,batch_size=1,sampler=valid_sampler,num_workers=4)\n",
    "\n",
    "# hack: this image format (1388, 1040) occurs only ones, stratify complains .. \n",
    "#dset.data_df.drop(dset.data_df[dset.data_df['size'] == (1388, 1040)].index[0])\n",
    "dset.data_df = dset.data_df[dset.data_df['size'] != (1388, 1040)]\n",
    "#idx = np.where(dset.data_df['size'] == (1388, 1040))\n",
    "\n",
    "stratify = dset.data_df['images'].map(lambda x: '{}'.format(x.size))\n",
    "train_dset, valid_dset = dset.train_test_split(test_size=0.05, random_state=1, shuffle=True, stratify=stratify)\n",
    "train_loader = DataLoader(train_dset, batch_size=1,shuffle=True)\n",
    "valid_loader = DataLoader(valid_dset, batch_size=1,shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "format  mode  size        \n",
       "PNG     RGBA  (256, 256)      334\n",
       "              (320, 256)      112\n",
       "              (347, 260)        5\n",
       "              (360, 360)       91\n",
       "              (640, 512)       13\n",
       "              (696, 520)       92\n",
       "              (1024, 1024)     16\n",
       "              (1272, 603)       6\n",
       "              (1388, 1040)      1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary of image formats\n",
    "dset.data_df.groupby(['format','mode','size']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(256, 256) (320, 256) (347, 260) (360, 360) (640, 512) (696, 520)\n",
      " (1024, 1024) (1272, 603) (1388, 1040)]\n"
     ]
    }
   ],
   "source": [
    "modes = dset.data_df['images'].map(lambda x: x.mode)\n",
    "sizes = dset.data_df['images'].map(lambda x: x.size)\n",
    "print np.unique(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%writefile architectures.py\n",
    "#%pycat architectures.py\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)[source]\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, stride=1, kernel_size=3, padding=1),\n",
    "            #nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, stride=1, padding=1))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, stride=1, kernel_size=3, padding=1),\n",
    "            #nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, stride=1, padding=1))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, stride=1, kernel_size=3, padding=1),\n",
    "            #nn.BatchNorm2d(16),\n",
    "            nn.ReLU())\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(16, 1, stride=1, kernel_size=1, padding=0),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pycat main.py\n",
    "#%%writefile main.py\n",
    "\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def save_model(\n",
    "        model,\n",
    "        optimizer,\n",
    "        stats,\n",
    "        it,\n",
    "        is_best=False,\n",
    "        fname='model_save.pth.tar'):\n",
    "    m_state_dict = model.state_dict()\n",
    "    o_state_dict = optimizer.state_dict()\n",
    "\n",
    "    torch.save({\n",
    "        'it': it,\n",
    "        'model_state_dict': m_state_dict,\n",
    "        'optimizer_state_dict': o_state_dict,\n",
    "        'stats': stats},\n",
    "        fname)\n",
    "    if is_best:\n",
    "        shutil.copyfile(fname, 'model_best.pth.tar')\n",
    "\n",
    "\n",
    "def load_model(model, optimizer, fname='model_best.pth.tar'):\n",
    "\n",
    "    if os.path.isfile(fname):\n",
    "        checkpoint = torch.load(fname)\n",
    "        it = checkpoint['it']\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        stats = checkpoint['stats']\n",
    "        print(\n",
    "            \"=> loaded checkpoint '{}' (epoch {})\".format(\n",
    "                fname, checkpoint['epoch']))\n",
    "        return it, stats\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(fname))\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    cnt = 0\n",
    "    for i, (img, labels) in enumerate(loader):\n",
    "        img, labels = Variable(img), Variable(labels)\n",
    "        outputs = model(img)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.data[0]\n",
    "        cnt = cnt + 1\n",
    "    l = running_loss / cnt\n",
    "    return l\n",
    "\n",
    "\n",
    "def train(\n",
    "        train_loader,\n",
    "        valid_loader,\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        stats,\n",
    "        best_loss,\n",
    "        best_it,\n",
    "        epoch,\n",
    "        it,\n",
    "        eval_every,\n",
    "        print_every,\n",
    "        save_every):\n",
    "    running_loss = 0.0\n",
    "    cnt = 0\n",
    "    for i, (img, labels) in enumerate(train_loader, it + 1):\n",
    "        img, labels = Variable(img), Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        model.train(True)\n",
    "\n",
    "        outputs = model(img)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cnt = cnt + 1\n",
    "\n",
    "        running_loss += loss.data[0]\n",
    "        if cnt > 0 and i % eval_every == 0:\n",
    "\n",
    "            l = validate(model, valid_loader, criterion)\n",
    "            stats.append((i, running_loss / cnt, l))\n",
    "            running_loss = 0.0\n",
    "\n",
    "            if cnt > 0 and i % print_every == 0:\n",
    "                print('[%d, %d]\\ttrain loss: %.3f\\tvalid loss: %.3f' %\n",
    "                      (epoch, i, stats[-1][1], stats[-1][2]))\n",
    "            if i % save_every == 0:\n",
    "                is_best = False\n",
    "                if best_loss > l:\n",
    "                    best_loss = l\n",
    "                    best_it = i\n",
    "                    is_best = True\n",
    "                    save_model(\n",
    "                        model,\n",
    "                        optimizer,\n",
    "                        stats,\n",
    "                        i,\n",
    "                        is_best,\n",
    "                        'model_save.pth.tar')\n",
    "    return i, best_loss, best_it\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, lr0):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = lr0 * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'to_tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-323-4e7780e37b5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# adjust_learning_rate(optimizer, epoch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_it\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_it\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_every\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-283-d1b074ef3d54>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, valid_loader, model, criterion, optimizer, stats, best_loss, best_it, epoch, it, eval_every, print_every, save_every)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mcnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/stefan/anaconda/lib/python2.7/site-packages/torch/utils/data/dataloader.pyc\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-303-0cc33c8728ac>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-312-d80b00720492>\u001b[0m in \u001b[0;36mtrain_transform\u001b[0;34m(img, mask)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m#img = torch.from_numpy(np.transpose(img,(2,0,1))).float()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m#mask = torch.from_numpy(np.transpose(mask,(2,0,1))).float()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'to_tensor'"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "model = CNN()\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001,momentum=0.9, weight_decay=1e-4)\n",
    "#optimizer = optim.Adam(model.parameters(),lr=0.0001,weight_decay=1e-4)\n",
    "\n",
    "\n",
    "print_every = 10\n",
    "save_every = 10\n",
    "eval_every = 10\n",
    "    \n",
    "epochs = 10\n",
    "\n",
    "it = 0\n",
    "best_loss = 1e20\n",
    "best_it = 0\n",
    "stats = []\n",
    "for epoch in range(epochs):\n",
    "    # adjust_learning_rate(optimizer, epoch)\n",
    "    it, best_loss, best_it = train(train_loader, valid_loader, model, criterion, optimizer, stats, best_loss, best_it, epoch, it, eval_every, print_every, save_every)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256]) 2.0 123.0 0.0 255.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-328-28f717ed5f54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#train_dset.transform=transforms.ToTensor()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#train_dset.transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mTT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToPILImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/stefan/anaconda/lib/python2.7/site-packages/torchvision-0.1.9-py2.7.egg/torchvision/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mpic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mnpimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pic should be Tensor or ndarray'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnpimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/stefan/anaconda/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(a, axes)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \"\"\"\n\u001b[0;32m--> 550\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'transpose'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/stefan/anaconda/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as TT\n",
    "#monkeypatch(dset, 'transform', train_transform)\n",
    "train_dset.transform=train_transform\n",
    "valid_dset.transform=train_transform\n",
    "#for i,(img,mask) in enumerate(train_loader):\n",
    "#    break\n",
    "\n",
    "#img = F.to_pil_image(img)\n",
    "#mask = F.to_pil_image(mask)\n",
    "print img.shape, mask.shape, img.min(), img.max(), mask.min(), mask.max()\n",
    "#model.eval()\n",
    "#print type(img)\n",
    "#pred = model(Variable(img,requires_grad=False))\n",
    "#show_img([img,mask,pred.data])\n",
    "#plot_img_and_hist(mask)\n",
    "mask.mean()\n",
    "mask.shape\n",
    "#show_img([img,mask])\n",
    "#train_dset.transform\n",
    "#train_dset.transform=transforms.ToTensor()\n",
    "#train_dset.transform\n",
    "TT.ToPILImage()(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'it'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-77d74b8fa31f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-142-8df94ae977be>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(model, optimizer, fname)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'it'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'it'"
     ]
    }
   ],
   "source": [
    "epoch,stats=load_model(model, optimizer)\n",
    "print epoch, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__all__',\n",
       " '__builtins__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '_range',\n",
       " 'btriunpack',\n",
       " 'chunk',\n",
       " 'matmul',\n",
       " 'mul',\n",
       " 'reduce',\n",
       " 'split',\n",
       " 'stack',\n",
       " 'torch',\n",
       " 'unbind']"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
